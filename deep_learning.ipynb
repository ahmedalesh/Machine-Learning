{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1./(1 +np.exp(-z))\n",
    "\n",
    "def Relu(z):\n",
    "    s = np.maximum(0,z)\n",
    "    return s\n",
    "\n",
    "def initialize_parameter(layer_dims):\n",
    "    L = len(layer_dims)\n",
    "    parameters = {}\n",
    "    for l in range(1,L):\n",
    "        parameters['W'+str(l)] = np.random.rand(layer_dims[l], layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(W,X,b):\n",
    "    Z = np.dot(W,X) + b\n",
    "    return Z\n",
    "\n",
    "def linear_forward_activation(parameters, X, L, activation='Relu'):\n",
    "    cache = {}\n",
    "    cache['A0'] = X\n",
    "    cache['Z0'] = np.zeros(X.shape)\n",
    "    for l in range(1,L-1):\n",
    "        cache['Z' + str(l)] = linear_forward(parameters['W'+str(l)], cache['A' +str(l-1)], parameters['b'+str(l)])\n",
    "        if(activation == 'Relu'):\n",
    "            cache['A' + str(l)] = Relu(cache['Z' + str(l)])\n",
    "        else:\n",
    "             cache['A' + str(l)] =  cache['Z' + str(l)]\n",
    "    cache['Z' + str(L-1)] = linear_forward(parameters['W'+str(L-1)],cache['A' +str(l)],parameters['b'+str(L-1)])\n",
    "    cache['A'+str(L-1)] = sigmoid(cache['Z'+str(L-1)])\n",
    "    return cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_cost(A,Y):\n",
    "    cost = np.sum((-Y*np.log(A)) - ((1-Y) * np.log(1-A)))\n",
    "    cost = 1./float(Y.shape[1]) * cost\n",
    "    return cost\n",
    "\n",
    "def linear_backward_propagation(parameters, cache, Y, L, activation = 'Relu'):\n",
    "    cache['dZ' + str(L-1)] = cache['A' + str(L-1)] - Y\n",
    "    gradients = {}\n",
    "    for l in reversed(range(1,L)):\n",
    "        gradients['dw' + str(l)] = np.dot(cache['dZ' + str(l)],cache['A' + str(l-1)].T)\n",
    "        gradients['db' + str(l)] = np.sum( cache['dZ' + str(l)], axis = 1, keepdims = True)  \n",
    "        cache['dA' + str(l-1)] =  np.dot(parameters['W' + str(l)].T, cache['dZ' + str(l)])\n",
    "        if activation == 'Relu':\n",
    "            cache['dZ' + str(l-1)] = relu_backward(cache['dA' + str(l-1)],cache['Z' + str(l-1)])\n",
    "        elif activation == 'sigmoid':\n",
    "            cache['dZ' + str(l-1)] =  sigmoid_backward(cache['dA' + str(l-1)],cache['Z' + str(l-1)])\n",
    "        elif activation == 'linear':\n",
    "            cache['dZ' + str(l-1)] = np.array(cache['dA' + str(l-1)], copy=True)\n",
    "    return cache,gradients\n",
    "\n",
    "def update_parameters(parameters, L, gradient, learning_rate):\n",
    "    for l in range(1,L):\n",
    "        parameters['W' + str(l)] = parameters['W' + str(l)] - learning_rate * gradient['dw'+str(l)]\n",
    "        parameters['b' + str(l)] = parameters['b' + str(l)] - learning_rate * gradient['db'+str(l)]\n",
    "    return parameters\n",
    "\n",
    "def L_model_network(layer_dims, X, Y, learning_rate, no_of_iterations, activation = 'Relu'):\n",
    "    L = len(layer_dims)\n",
    "    parameters = initialize_parameter(layer_dims)\n",
    "    for i in range(no_of_iterations):\n",
    "        cache = linear_forward_activation(parameters,X, L, activation)\n",
    "        cache, gradient = linear_backward_propagation(parameters, cache, Y, L, activation)\n",
    "        parameters = update_parameters(parameters, L, gradient, learning_rate)\n",
    "    return parameters  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)\n",
    "\n",
    "    \n",
    "    return Z2\n",
    "\n",
    "def compute_cost(Y, Z2):\n",
    "    logits = tf.transpose(Z2)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    return cost\n",
    "\n",
    "def initializer_parameters():\n",
    "    W1 = tf.get_variable(\"W1\", [4,8], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [4,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [1,4], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [1,1], initializer = tf.zeros_initializer())\n",
    "   \n",
    "    parameters = {\"W1\" : W1,\n",
    "                 \"W2\": W2,\n",
    "                 \"b1\": b1,\n",
    "                 \"b2\": b2}\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(train_X, train_Y, learning_rate= 0.001, n_epochs = 2000):\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape =(8,None))\n",
    "    Y = tf.placeholder(tf.float32, shape=(1,None))\n",
    "    parameters = initializer_parameters()\n",
    "    \n",
    "    Z2 = forward_propagation(X,parameters)\n",
    "    \n",
    "    cost = compute_cost(Y,Z2)\n",
    "    optimizer =tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    final_cost = []\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        epoch_cost = 0\n",
    "        for i in range(n_epochs):\n",
    "            sess.run(optimizer, feed_dict={X:train_X,Y:train_Y})\n",
    "            final_cost.append(cost.eval({X: train_X, Y: train_Y}))\n",
    "        plt.plot(range(n_epochs),final_cost)\n",
    "        plt.show()\n",
    "        parameters = sess.run(parameters)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.nn.sigmoid(Z2),Y)\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: train_X, Y: train_Y}))\n",
    "        \n",
    "        return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
